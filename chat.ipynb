{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot initialized! Type 'quit' to exit, 'history' to view conversation history, or 'clear' to clear history.\n",
      "\n",
      "Assistant: Hello Badreddine! I'm doing well, thank you for asking. I'm a large language model, so I don't have feelings or emotions like humans do, but I'm always happy to chat with new friends.\n",
      "\n",
      "It's lovely to meet you, Badreddine! Is there something on your mind that you'd like to talk about, or would you like some conversation starters?\n",
      "\n",
      "Assistant: I think I see what's going on here! Your name is actually \"Badreddine\", and we've already established that. But just to confirm, I'll repeat it back: your name is Badreddine. Is there something specific you'd like to know or talk about related to your name?\n",
      "\n",
      "Assistant: I think we've had this conversation before! Your name is still \"Badreddine\". If you're asking for confirmation, I'm happy to provide it: yes, your name is indeed Badreddine. Is there something else on your mind that you'd like to talk about?\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# First cell - Import required libraries\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import faiss\n",
    "import ollama\n",
    "from ollama import Options\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Second cell - Create the ConversationManager class\n",
    "class ConversationManager:\n",
    "    def __init__(self, model_name: str):\n",
    "        # Initialize the sentence transformer for creating embeddings\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.embedding_size = 384\n",
    "        self.index = faiss.IndexFlatL2(self.embedding_size)\n",
    "        self.conversations = []\n",
    "        self.ollama_model = model_name\n",
    "    \n",
    "    def add_message(self, message: str, role: str = 'user'):\n",
    "        \"\"\"Add a message to the conversation history and create its embedding\"\"\"\n",
    "        inputs = self.tokenizer(message, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        embeddings = outputs.pooler_output.cpu().detach().numpy()\n",
    "        self.index.add(embeddings)\n",
    "        self.conversations.append({'role': role, 'content': message})\n",
    "    \n",
    "    def generate_response(self, message: str, temperature: float = 0.7) -> str:\n",
    "        \"\"\"Generate a response using Ollama and maintain conversation history\"\"\"\n",
    "        self.add_message(message, 'user')\n",
    "        \n",
    "        # Use the last 10 messages for context\n",
    "        messages = self.conversations[-10:]\n",
    "        \n",
    "        options = Options(\n",
    "            temperature=temperature,\n",
    "            num_ctx=8192,\n",
    "            num_predict=100,\n",
    "        )\n",
    "        \n",
    "        response = ollama.chat(\n",
    "            model=self.ollama_model,\n",
    "            messages=messages,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        new_message_content = response['message']['content']\n",
    "        self.add_message(new_message_content, 'assistant')\n",
    "        return new_message_content\n",
    "    \n",
    "    def view_conversation_history(self):\n",
    "        \"\"\"Display the conversation history\"\"\"\n",
    "        for i, msg in enumerate(self.conversations):\n",
    "            print(f\"{i+1}. {msg['role'].title()}: {msg['content']}\\n\")\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear the conversation history\"\"\"\n",
    "        self.conversations = []\n",
    "        self.index = faiss.IndexFlatL2(self.embedding_size)\n",
    "\n",
    "# Third cell - Initialize the chatbot\n",
    "# Replace 'model_name' with your preferred Ollama model\n",
    "chatbot = ConversationManager(model_name='llama3.2:3b')  # or any other model you have in Ollama\n",
    "\n",
    "# Fourth cell - Create a simple chat interface\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def chat():\n",
    "    print(\"Chatbot initialized! Type 'quit' to exit, 'history' to view conversation history, or 'clear' to clear history.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        elif user_input.lower() == 'history':\n",
    "            chatbot.view_conversation_history()\n",
    "            continue\n",
    "        elif user_input.lower() == 'clear':\n",
    "            chatbot.clear_history()\n",
    "            clear_output(wait=True)\n",
    "            print(\"History cleared! Type 'quit' to exit, 'history' to view conversation history, or 'clear' to clear history.\")\n",
    "            continue\n",
    "        elif user_input == \"\":\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            response = chatbot.generate_response(user_input)\n",
    "            print(f\"\\nAssistant: {response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")\n",
    "\n",
    "# Fifth cell - Start the chat\n",
    "# Run this cell to start chatting\n",
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Chatbot initialized! Commands:\n",
      "- 'quit': Exit chat\n",
      "- 'history': View conversation history\n",
      "- 'clear': Clear history\n",
      "- 'save': Save conversation to file\n",
      "- 'load': Load conversation from file\n",
      "\n",
      "Assistant: Hello Badreddine! I'm just a language model, so I don't have feelings or emotions like humans do, but I'm here to help and chat with you. It's great to meet you!\n",
      "\n",
      "How about you? What brings you here today? Do you have any questions or topics you'd like to discuss?\n",
      "\n",
      "Assistant: Badreddine! Yes, I do remember your name. You mentioned it earlier when we started chatting. How can I assist you today?\n",
      "1. User: hello how are you my name is badreddine\n",
      "\n",
      "2. Assistant: Hello Badreddine! I'm just a language model, so I don't have feelings or emotions like humans do, but I'm here to help and chat with you. It's great to meet you!\n",
      "\n",
      "How about you? What brings you here today? Do you have any questions or topics you'd like to discuss?\n",
      "\n",
      "3. User: do you remember my name?\n",
      "\n",
      "4. Assistant: Badreddine! Yes, I do remember your name. You mentioned it earlier when we started chatting. How can I assist you today?\n",
      "\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# First cell - Import required libraries\n",
    "import ollama\n",
    "from ollama import Options\n",
    "from typing import List, Dict, Any, Optional\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class SimpleConversationManager:\n",
    "    def __init__(self, model_name: str):\n",
    "        \"\"\"Initialize conversation manager with specified Ollama model\"\"\"\n",
    "        self.conversations = []  # Store conversation history\n",
    "        self.ollama_model = model_name\n",
    "    \n",
    "    def add_message(self, message: str, role: str = 'user'):\n",
    "        \"\"\"Add a message to conversation history\"\"\"\n",
    "        self.conversations.append({'role': role, 'content': message})\n",
    "    \n",
    "    def generate_response(self, message: str, temperature: float = 0.7, \n",
    "                         context_window: int = 10) -> str:\n",
    "        \"\"\"Generate response using simple context window\"\"\"\n",
    "        self.add_message(message, 'user')\n",
    "        \n",
    "        # Get last n messages for context\n",
    "        messages = self.conversations[-context_window:]\n",
    "        \n",
    "        options = Options(\n",
    "            temperature=temperature,\n",
    "            num_ctx=8192,\n",
    "            num_predict=100,\n",
    "        )\n",
    "        \n",
    "        response = ollama.chat(\n",
    "            model=self.ollama_model,\n",
    "            messages=messages,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        new_message_content = response['message']['content']\n",
    "        self.add_message(new_message_content, 'assistant')\n",
    "        return new_message_content\n",
    "    \n",
    "    def view_conversation_history(self):\n",
    "        \"\"\"Display the conversation history\"\"\"\n",
    "        for i, msg in enumerate(self.conversations):\n",
    "            print(f\"{i+1}. {msg['role'].title()}: {msg['content']}\\n\")\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear the conversation history\"\"\"\n",
    "        self.conversations = []\n",
    "    \n",
    "    def save_conversations(self, filename: str):\n",
    "        \"\"\"Save conversations to file\"\"\"\n",
    "        import json\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.conversations, f)\n",
    "    \n",
    "    def load_conversations(self, filename: str):\n",
    "        \"\"\"Load conversations from file\"\"\"\n",
    "        import json\n",
    "        with open(filename, 'r') as f:\n",
    "            self.conversations = json.load(f)\n",
    "\n",
    "def simple_chat():\n",
    "    # Initialize chatbot\n",
    "    chatbot = SimpleConversationManager(model_name='llama3.2:3b')\n",
    "    \n",
    "    print(\"Simple Chatbot initialized! Commands:\")\n",
    "    print(\"- 'quit': Exit chat\")\n",
    "    print(\"- 'history': View conversation history\")\n",
    "    print(\"- 'clear': Clear history\")\n",
    "    print(\"- 'save': Save conversation to file\")\n",
    "    print(\"- 'load': Load conversation from file\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        elif user_input.lower() == 'history':\n",
    "            chatbot.view_conversation_history()\n",
    "            continue\n",
    "        elif user_input.lower() == 'clear':\n",
    "            chatbot.clear_history()\n",
    "            clear_output(wait=True)\n",
    "            print(\"History cleared!\")\n",
    "            continue\n",
    "        elif user_input.lower() == 'save':\n",
    "            filename = input(\"Enter filename to save: \")\n",
    "            chatbot.save_conversations(filename)\n",
    "            print(f\"Conversations saved to {filename}\")\n",
    "            continue\n",
    "        elif user_input.lower() == 'load':\n",
    "            filename = input(\"Enter filename to load: \")\n",
    "            chatbot.load_conversations(filename)\n",
    "            print(f\"Conversations loaded from {filename}\")\n",
    "            continue\n",
    "        elif user_input == \"\":\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            response = chatbot.generate_response(user_input)\n",
    "            print(f\"\\nAssistant: {response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")\n",
    "\n",
    "# Run this cell to start the simple chatbot\n",
    "simple_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Chatbot initialized! Commands:\n",
      "- 'quit': Exit chat\n",
      "- 'history': View conversation history\n",
      "- 'clear': Clear history\n",
      "- 'save': Save conversation to file\n",
      "- 'load': Load conversation from file\n",
      "\n",
      "Assistant: What's up Badreddine?! My name is not a personal one, I'm an AI assistant, so I don't have a personal name. I'm here to help answer any questions you may have or just chat with you! How's it going?\n",
      "Conversations saved to chat_1\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# First cell - Import required libraries\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import faiss\n",
    "import ollama\n",
    "from ollama import Options\n",
    "from typing import List, Dict, Any, Optional\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class SemanticConversationManager:\n",
    "    def __init__(self, model_name: str):\n",
    "        \"\"\"Initialize conversation manager with semantic search capabilities\"\"\"\n",
    "        # Initialize sentence transformer for embeddings\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.embedding_size = 384\n",
    "        self.index = faiss.IndexFlatL2(self.embedding_size)\n",
    "        self.conversations = []\n",
    "        self.ollama_model = model_name\n",
    "    \n",
    "    def _get_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Generate embedding for a text string\"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return outputs.pooler_output.cpu().detach().numpy()\n",
    "    \n",
    "    def add_message(self, message: str, role: str = 'user'):\n",
    "        \"\"\"Add a message and its embedding to storage\"\"\"\n",
    "        embedding = self._get_embedding(message)\n",
    "        self.index.add(embedding)\n",
    "        self.conversations.append({\n",
    "            'role': role, \n",
    "            'content': message,\n",
    "            'embedding_idx': len(self.conversations)\n",
    "        })\n",
    "    \n",
    "    def get_semantic_context(self, message: str, k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Retrieve most semantically similar messages\"\"\"\n",
    "        # Get embedding for current message\n",
    "        query_embedding = self._get_embedding(message)\n",
    "        \n",
    "        # Search for similar messages\n",
    "        D, I = self.index.search(query_embedding, k)\n",
    "        \n",
    "        # Get relevant messages in order of relevance\n",
    "        relevant_messages = [self.conversations[i] for i in I[0]]\n",
    "        \n",
    "        # Sort by original order to maintain conversation flow\n",
    "        relevant_messages.sort(key=lambda x: x['embedding_idx'])\n",
    "        \n",
    "        return relevant_messages\n",
    "    \n",
    "    def generate_response(self, message: str, temperature: float = 0.7,\n",
    "                         semantic_k: int = 5, include_recent: int = 3) -> str:\n",
    "        \"\"\"Generate response using semantic search and recent context\"\"\"\n",
    "        self.add_message(message, 'user')\n",
    "        \n",
    "        # Get semantically relevant messages\n",
    "        semantic_context = self.get_semantic_context(message, k=semantic_k)\n",
    "        \n",
    "        # Get most recent messages\n",
    "        recent_messages = self.conversations[-include_recent:]\n",
    "        \n",
    "        # Combine contexts (remove duplicates while preserving order)\n",
    "        seen = set()\n",
    "        context_messages = []\n",
    "        for msg in semantic_context + recent_messages:\n",
    "            if msg['embedding_idx'] not in seen:\n",
    "                seen.add(msg['embedding_idx'])\n",
    "                context_messages.append({\n",
    "                    'role': msg['role'],\n",
    "                    'content': msg['content']\n",
    "                })\n",
    "        \n",
    "        options = Options(\n",
    "            temperature=temperature,\n",
    "            num_ctx=8192,\n",
    "            num_predict=100,\n",
    "        )\n",
    "        \n",
    "        response = ollama.chat(\n",
    "            model=self.ollama_model,\n",
    "            messages=context_messages,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        new_message_content = response['message']['content']\n",
    "        self.add_message(new_message_content, 'assistant')\n",
    "        return new_message_content\n",
    "    \n",
    "    def view_conversation_history(self):\n",
    "        \"\"\"Display the conversation history\"\"\"\n",
    "        for i, msg in enumerate(self.conversations):\n",
    "            print(f\"{i+1}. {msg['role'].title()}: {msg['content']}\\n\")\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear the conversation history and FAISS index\"\"\"\n",
    "        self.conversations = []\n",
    "        self.index = faiss.IndexFlatL2(self.embedding_size)\n",
    "    \n",
    "    def save_conversations(self, filename: str):\n",
    "        \"\"\"Save conversations to file\"\"\"\n",
    "        import json\n",
    "        # Save only the essential conversation data\n",
    "        save_data = [{\n",
    "            'role': msg['role'],\n",
    "            'content': msg['content']\n",
    "        } for msg in self.conversations]\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(save_data, f)\n",
    "    \n",
    "    def load_conversations(self, filename: str):\n",
    "        \"\"\"Load conversations from file and rebuild FAISS index\"\"\"\n",
    "        import json\n",
    "        with open(filename, 'r') as f:\n",
    "            conversations = json.load(f)\n",
    "        \n",
    "        # Clear existing data\n",
    "        self.clear_history()\n",
    "        \n",
    "        # Rebuild conversations and index\n",
    "        for msg in conversations:\n",
    "            self.add_message(msg['content'], msg['role'])\n",
    "\n",
    "def semantic_chat():\n",
    "    # Initialize chatbot\n",
    "    chatbot = SemanticConversationManager(model_name='llama3.2:3b')\n",
    "    \n",
    "    print(\"Semantic Chatbot initialized! Commands:\")\n",
    "    print(\"- 'quit': Exit chat\")\n",
    "    print(\"- 'history': View conversation history\")\n",
    "    print(\"- 'clear': Clear history\")\n",
    "    print(\"- 'save': Save conversation to file\")\n",
    "    print(\"- 'load': Load conversation from file\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        elif user_input.lower() == 'history':\n",
    "            chatbot.view_conversation_history()\n",
    "            continue\n",
    "        elif user_input.lower() == 'clear':\n",
    "            chatbot.clear_history()\n",
    "            clear_output(wait=True)\n",
    "            print(\"History cleared!\")\n",
    "            continue\n",
    "        elif user_input.lower() == 'save':\n",
    "            filename = input(\"Enter filename to save: \")\n",
    "            chatbot.save_conversations(filename)\n",
    "            print(f\"Conversations saved to {filename}\")\n",
    "            continue\n",
    "        elif user_input.lower() == 'load':\n",
    "            filename = input(\"Enter filename to load: \")\n",
    "            chatbot.load_conversations(filename)\n",
    "            print(f\"Conversations loaded from {filename}\")\n",
    "            continue\n",
    "        elif user_input == \"\":\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            response = chatbot.generate_response(user_input)\n",
    "            print(f\"\\nAssistant: {response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")\n",
    "\n",
    "# Run this cell to start the semantic chatbot\n",
    "semantic_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF-Enhanced Chatbot initialized! Commands:\n",
      "- 'quit': Exit chat\n",
      "- 'history': View conversation history\n",
      "- 'clear': Clear history\n",
      "- 'save': Save conversation to file\n",
      "- 'load': Load conversation from file\n",
      "- 'pdf': Load a PDF file\n",
      "PDF loaded successfully. Total length: 8024 characters\n",
      "\n",
      "Assistant: The names of the two persons who had the accident are:\n",
      "\n",
      "1. Badreddine Hannaoui\n",
      "2. Maxence Ratignier\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# First cell - Import required libraries\n",
    "import ollama\n",
    "from ollama import Options\n",
    "from typing import List, Dict, Any, Optional\n",
    "from IPython.display import clear_output\n",
    "import PyPDF2\n",
    "import re\n",
    "\n",
    "class PDFEnhancedConversationManager:\n",
    "    def __init__(self, model_name: str):\n",
    "        \"\"\"Initialize conversation manager with specified Ollama model\"\"\"\n",
    "        self.conversations = []  # Store conversation history\n",
    "        self.ollama_model = model_name\n",
    "        self.pdf_content = None  # Store PDF content\n",
    "        self.system_prompt = None  # Store system prompt\n",
    "    \n",
    "    def load_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"Load and parse PDF content\"\"\"\n",
    "        try:\n",
    "            pdf_text = []\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                for page in pdf_reader.pages:\n",
    "                    text = page.extract_text()\n",
    "                    # Clean the text\n",
    "                    text = re.sub(r'\\s+', ' ', text)\n",
    "                    pdf_text.append(text)\n",
    "            \n",
    "            self.pdf_content = \" \".join(pdf_text)\n",
    "            \n",
    "            # Create system prompt with PDF content\n",
    "            self.system_prompt = {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"You have been provided with the following document content. \n",
    "                When answering questions, use this content as context and provide accurate responses based on it.\n",
    "                If the answer cannot be found in the document, clearly state that.\n",
    "                \n",
    "                Document content:\n",
    "                {self.pdf_content[:2000]}  # First 2000 chars for initial context\n",
    "                \n",
    "                Additional context will be provided in the conversation history.\"\"\"\n",
    "            }\n",
    "            \n",
    "            # Add chunked PDF content to conversation history\n",
    "            chunk_size = 2000\n",
    "            for i in range(0, len(self.pdf_content), chunk_size):\n",
    "                chunk = self.pdf_content[i:i + chunk_size]\n",
    "                self.conversations.append({\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"Document content continued: {chunk}\"\n",
    "                })\n",
    "            \n",
    "            return f\"PDF loaded successfully. Total length: {len(self.pdf_content)} characters\"\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error loading PDF: {str(e)}\")\n",
    "    \n",
    "    def add_message(self, message: str, role: str = 'user'):\n",
    "        \"\"\"Add a message to conversation history\"\"\"\n",
    "        self.conversations.append({'role': role, 'content': message})\n",
    "    \n",
    "    def generate_response(self, message: str, temperature: float = 0.7, \n",
    "                         context_window: int = 10) -> str:\n",
    "        \"\"\"Generate response using simple context window\"\"\"\n",
    "        self.add_message(message, 'user')\n",
    "        \n",
    "        # Get context messages\n",
    "        messages = []\n",
    "        if self.system_prompt:\n",
    "            messages.append(self.system_prompt)\n",
    "        \n",
    "        # Add recent conversation history\n",
    "        messages.extend(self.conversations[-context_window:])\n",
    "        \n",
    "        options = Options(\n",
    "            temperature=temperature,\n",
    "            num_ctx=8192,\n",
    "            num_predict=100,\n",
    "        )\n",
    "        \n",
    "        response = ollama.chat(\n",
    "            model=self.ollama_model,\n",
    "            messages=messages,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        new_message_content = response['message']['content']\n",
    "        self.add_message(new_message_content, 'assistant')\n",
    "        return new_message_content\n",
    "    \n",
    "    def view_conversation_history(self):\n",
    "        \"\"\"Display the conversation history\"\"\"\n",
    "        for i, msg in enumerate(self.conversations):\n",
    "            if msg['role'] != 'system':  # Skip system messages for cleaner output\n",
    "                print(f\"{i+1}. {msg['role'].title()}: {msg['content']}\\n\")\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear the conversation history and PDF content\"\"\"\n",
    "        self.conversations = []\n",
    "        self.pdf_content = None\n",
    "        self.system_prompt = None\n",
    "    \n",
    "    def save_conversations(self, filename: str):\n",
    "        \"\"\"Save conversations to file\"\"\"\n",
    "        import json\n",
    "        save_data = {\n",
    "            'conversations': self.conversations,\n",
    "            'pdf_content': self.pdf_content\n",
    "        }\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(save_data, f)\n",
    "    \n",
    "    def load_conversations(self, filename: str):\n",
    "        \"\"\"Load conversations from file\"\"\"\n",
    "        import json\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            self.conversations = data['conversations']\n",
    "            self.pdf_content = data['pdf_content']\n",
    "            if self.pdf_content:\n",
    "                # Recreate system prompt\n",
    "                self.system_prompt = {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"\"\"You have been provided with the following document content. \n",
    "                    When answering questions, use this content as context and provide accurate responses based on it.\n",
    "                    If the answer cannot be found in the document, clearly state that.\n",
    "                    \n",
    "                    Document content:\n",
    "                    {self.pdf_content[:2000]}\"\"\"\n",
    "                }\n",
    "\n",
    "def pdf_chat():\n",
    "    # Initialize chatbot\n",
    "    chatbot = PDFEnhancedConversationManager(model_name='llama3.2:3b')\n",
    "    \n",
    "    print(\"PDF-Enhanced Chatbot initialized! Commands:\")\n",
    "    print(\"- 'quit': Exit chat\")\n",
    "    print(\"- 'history': View conversation history\")\n",
    "    print(\"- 'clear': Clear history\")\n",
    "    print(\"- 'save': Save conversation to file\")\n",
    "    print(\"- 'load': Load conversation from file\")\n",
    "    print(\"- 'pdf': Load a PDF file\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        elif user_input.lower() == 'history':\n",
    "            chatbot.view_conversation_history()\n",
    "            continue\n",
    "        elif user_input.lower() == 'clear':\n",
    "            chatbot.clear_history()\n",
    "            clear_output(wait=True)\n",
    "            print(\"History cleared!\")\n",
    "            continue\n",
    "        elif user_input.lower() == 'save':\n",
    "            filename = input(\"Enter filename to save: \")\n",
    "            chatbot.save_conversations(filename)\n",
    "            print(f\"Conversations saved to {filename}\")\n",
    "            continue\n",
    "        elif user_input.lower() == 'load':\n",
    "            filename = input(\"Enter filename to load: \")\n",
    "            chatbot.load_conversations(filename)\n",
    "            print(f\"Conversations loaded from {filename}\")\n",
    "            continue\n",
    "        elif user_input.lower() == 'pdf':\n",
    "            pdf_path = input(\"Enter the path to your PDF file: \")\n",
    "            try:\n",
    "                result = chatbot.load_pdf(pdf_path)\n",
    "                print(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading PDF: {str(e)}\")\n",
    "            continue\n",
    "        elif user_input == \"\":\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            response = chatbot.generate_response(user_input)\n",
    "            print(f\"\\nAssistant: {response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")\n",
    "\n",
    "# Run this cell to start the PDF-enhanced chatbot\n",
    "pdf_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ebb929f2774fd2957aab6513deff2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>PDF-Enhanced Chatbot</h3>'), FileUpload(value=(), accept='.pdf', description='U…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive Chatbot initialized! Commands:\n",
      "- 'quit': Exit chat\n",
      "- 'history': View conversation history\n",
      "- 'clear': Clear history\n",
      "- 'save': Save conversation to file\n",
      "- 'load': Load conversation from file\n",
      "\n",
      "Use the upload widget above to load a PDF file.\n",
      "\n",
      "Assistant: Hello Badreddine! I'm doing well, thank you for asking. It's great to meet you! I'm a large language model, so I don't have emotions or feelings like humans do, but I'm always happy to chat with someone new and help with any questions or topics you'd like to discuss.\n",
      "\n",
      "How about you? How's your day going so far?\n",
      "\n",
      "Assistant: Badreddine is a unique and interesting name. Yes, I do remember it - we just started our conversation, and I've been keeping track of the names mentioned. So, feel free to come back and chat with me anytime, Badreddine!\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# First cell - Import required libraries\n",
    "import ollama\n",
    "from ollama import Options\n",
    "from typing import List, Dict, Any, Optional\n",
    "from IPython.display import clear_output, display\n",
    "import ipywidgets as widgets\n",
    "import PyPDF2\n",
    "import re\n",
    "import io\n",
    "\n",
    "class InteractivePDFChatbot:\n",
    "    def __init__(self, model_name: str):\n",
    "        \"\"\"Initialize conversation manager with specified Ollama model\"\"\"\n",
    "        self.conversations = []\n",
    "        self.ollama_model = model_name\n",
    "        self.pdf_content = None\n",
    "        self.system_prompt = None\n",
    "        self.setup_upload_widget()\n",
    "    \n",
    "    def setup_upload_widget(self):\n",
    "        \"\"\"Create and configure the upload widget\"\"\"\n",
    "        self.upload_widget = widgets.FileUpload(\n",
    "            accept='.pdf',  # Only accept PDF files\n",
    "            multiple=False,  # Single file upload\n",
    "            description='Upload PDF',\n",
    "            layout=widgets.Layout(width='300px')\n",
    "        )\n",
    "        self.upload_widget.observe(self._on_upload_change, names='value')\n",
    "        \n",
    "        # Create status output widget\n",
    "        self.status_output = widgets.Output()\n",
    "    \n",
    "    def _on_upload_change(self, change):\n",
    "        \"\"\"Handle PDF upload event\"\"\"\n",
    "        with self.status_output:\n",
    "            clear_output()\n",
    "            if len(change.new) > 0:\n",
    "                # Get the uploaded file\n",
    "                uploaded_file = next(iter(change.new.values()))\n",
    "                try:\n",
    "                    result = self.process_uploaded_pdf(uploaded_file)\n",
    "                    print(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing PDF: {str(e)}\")\n",
    "    \n",
    "    def process_uploaded_pdf(self, uploaded_file) -> str:\n",
    "        \"\"\"Process uploaded PDF file\"\"\"\n",
    "        try:\n",
    "            # Create a BytesIO object from uploaded content\n",
    "            pdf_stream = io.BytesIO(uploaded_file['content'])\n",
    "            \n",
    "            pdf_text = []\n",
    "            pdf_reader = PyPDF2.PdfReader(pdf_stream)\n",
    "            for page in pdf_reader.pages:\n",
    "                text = page.extract_text()\n",
    "                # Clean the text\n",
    "                text = re.sub(r'\\s+', ' ', text)\n",
    "                pdf_text.append(text)\n",
    "            \n",
    "            self.pdf_content = \" \".join(pdf_text)\n",
    "            \n",
    "            # Create system prompt with PDF content\n",
    "            self.system_prompt = {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"You have been provided with the following document content. \n",
    "                When answering questions, use this content as context and provide accurate responses based on it.\n",
    "                If the answer cannot be found in the document, clearly state that.\n",
    "                \n",
    "                Document content:\n",
    "                {self.pdf_content[:2000]}  # First 2000 chars for initial context\n",
    "                \n",
    "                Additional context will be provided in the conversation history.\"\"\"\n",
    "            }\n",
    "            \n",
    "            # Add chunked PDF content to conversation history\n",
    "            chunk_size = 2000\n",
    "            for i in range(0, len(self.pdf_content), chunk_size):\n",
    "                chunk = self.pdf_content[i:i + chunk_size]\n",
    "                self.conversations.append({\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"Document content continued: {chunk}\"\n",
    "                })\n",
    "            \n",
    "            return f\"PDF '{uploaded_file['metadata']['name']}' loaded successfully. Total length: {len(self.pdf_content)} characters\"\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error processing PDF: {str(e)}\")\n",
    "    \n",
    "    def add_message(self, message: str, role: str = 'user'):\n",
    "        \"\"\"Add a message to conversation history\"\"\"\n",
    "        self.conversations.append({'role': role, 'content': message})\n",
    "    \n",
    "    def generate_response(self, message: str, temperature: float = 0.7, \n",
    "                         context_window: int = 10) -> str:\n",
    "        \"\"\"Generate response using simple context window\"\"\"\n",
    "        self.add_message(message, 'user')\n",
    "        \n",
    "        # Get context messages\n",
    "        messages = []\n",
    "        if self.system_prompt:\n",
    "            messages.append(self.system_prompt)\n",
    "        \n",
    "        # Add recent conversation history\n",
    "        messages.extend(self.conversations[-context_window:])\n",
    "        \n",
    "        options = Options(\n",
    "            temperature=temperature,\n",
    "            num_ctx=8192,\n",
    "            num_predict=100,\n",
    "        )\n",
    "        \n",
    "        response = ollama.chat(\n",
    "            model=self.ollama_model,\n",
    "            messages=messages,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        new_message_content = response['message']['content']\n",
    "        self.add_message(new_message_content, 'assistant')\n",
    "        return new_message_content\n",
    "    \n",
    "    def view_conversation_history(self):\n",
    "        \"\"\"Display the conversation history\"\"\"\n",
    "        for i, msg in enumerate(self.conversations):\n",
    "            if msg['role'] != 'system':  # Skip system messages for cleaner output\n",
    "                print(f\"{i+1}. {msg['role'].title()}: {msg['content']}\\n\")\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear the conversation history and PDF content\"\"\"\n",
    "        self.conversations = []\n",
    "        self.pdf_content = None\n",
    "        self.system_prompt = None\n",
    "        # Clear upload widget\n",
    "        self.upload_widget.value.clear()\n",
    "        with self.status_output:\n",
    "            clear_output()\n",
    "            print(\"History and PDF content cleared!\")\n",
    "    \n",
    "    def save_conversations(self, filename: str):\n",
    "        \"\"\"Save conversations to file\"\"\"\n",
    "        import json\n",
    "        save_data = {\n",
    "            'conversations': self.conversations,\n",
    "            'pdf_content': self.pdf_content\n",
    "        }\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(save_data, f)\n",
    "    \n",
    "    def load_conversations(self, filename: str):\n",
    "        \"\"\"Load conversations from file\"\"\"\n",
    "        import json\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            self.conversations = data['conversations']\n",
    "            self.pdf_content = data['pdf_content']\n",
    "            if self.pdf_content:\n",
    "                # Recreate system prompt\n",
    "                self.system_prompt = {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"\"\"You have been provided with the following document content. \n",
    "                    When answering questions, use this content as context and provide accurate responses based on it.\n",
    "                    If the answer cannot be found in the document, clearly state that.\n",
    "                    \n",
    "                    Document content:\n",
    "                    {self.pdf_content[:2000]}\"\"\"\n",
    "                }\n",
    "\n",
    "def interactive_chat():\n",
    "    \"\"\"Initialize and run the interactive chatbot\"\"\"\n",
    "    # Initialize chatbot\n",
    "    chatbot = InteractivePDFChatbot(model_name='llama3.2:3b')\n",
    "    \n",
    "    # Display upload widget and status\n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML(\"<h3>PDF-Enhanced Chatbot</h3>\"),\n",
    "        chatbot.upload_widget,\n",
    "        chatbot.status_output\n",
    "    ]))\n",
    "    \n",
    "    print(\"Interactive Chatbot initialized! Commands:\")\n",
    "    print(\"- 'quit': Exit chat\")\n",
    "    print(\"- 'history': View conversation history\")\n",
    "    print(\"- 'clear': Clear history\")\n",
    "    print(\"- 'save': Save conversation to file\")\n",
    "    print(\"- 'load': Load conversation from file\")\n",
    "    print(\"\\nUse the upload widget above to load a PDF file.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        elif user_input.lower() == 'history':\n",
    "            chatbot.view_conversation_history()\n",
    "            continue\n",
    "        elif user_input.lower() == 'clear':\n",
    "            chatbot.clear_history()\n",
    "            continue\n",
    "        elif user_input.lower() == 'save':\n",
    "            filename = input(\"Enter filename to save: \")\n",
    "            chatbot.save_conversations(filename)\n",
    "            print(f\"Conversations saved to {filename}\")\n",
    "            continue\n",
    "        elif user_input.lower() == 'load':\n",
    "            filename = input(\"Enter filename to load: \")\n",
    "            chatbot.load_conversations(filename)\n",
    "            print(f\"Conversations loaded from {filename}\")\n",
    "            continue\n",
    "        elif user_input == \"\":\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            response = chatbot.generate_response(user_input)\n",
    "            print(f\"\\nAssistant: {response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")\n",
    "\n",
    "# Run this cell to start the interactive chatbot\n",
    "interactive_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83db413cd0da425a965bf0366deaa9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>PDF-Enhanced Chatbot</h3>'), FileUpload(value=(), accept='.pdf', description='U…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive Chatbot initialized! Commands:\n",
      "- 'pdf': Open file dialog to select PDF\n",
      "- 'quit': Exit chat\n",
      "- 'history': View conversation history\n",
      "- 'clear': Clear history\n",
      "- 'save': Save conversation to file\n",
      "- 'load': Load conversation from file\n",
      "\n",
      "Use the upload widget above or type 'pdf' to load a PDF file.\n",
      "PDF 'e-constat-auto_20241014-XGQTE.pdf' loaded successfully. Total length: 8024 characters\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from ollama import Options\n",
    "from typing import List, Dict, Any, Optional\n",
    "from IPython.display import clear_output, display\n",
    "import ipywidgets as widgets\n",
    "import PyPDF2\n",
    "import re\n",
    "import io\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import os\n",
    "\n",
    "class InteractivePDFChatbot:\n",
    "    def __init__(self, model_name: str):\n",
    "        \"\"\"Initialize conversation manager with specified Ollama model\"\"\"\n",
    "        self.conversations = []\n",
    "        self.ollama_model = model_name\n",
    "        self.pdf_content = None\n",
    "        self.system_prompt = None\n",
    "        self.setup_upload_widget()\n",
    "        \n",
    "        # Initialize tkinter but hide the main window\n",
    "        self.root = tk.Tk()\n",
    "        self.root.withdraw()\n",
    "    \n",
    "    def setup_upload_widget(self):\n",
    "        \"\"\"Create and configure the upload widget\"\"\"\n",
    "        self.upload_widget = widgets.FileUpload(\n",
    "            accept='.pdf',\n",
    "            multiple=False,\n",
    "            description='Upload PDF',\n",
    "            layout=widgets.Layout(width='300px')\n",
    "        )\n",
    "        self.upload_widget.observe(self._on_upload_change, names='value')\n",
    "        self.status_output = widgets.Output()\n",
    "    \n",
    "    def _on_upload_change(self, change):\n",
    "        \"\"\"Handle PDF upload event from widget\"\"\"\n",
    "        with self.status_output:\n",
    "            clear_output()\n",
    "            if len(change.new) > 0:\n",
    "                # Get the uploaded file\n",
    "                uploaded_file = next(iter(change.new.values()))\n",
    "                try:\n",
    "                    result = self.process_uploaded_pdf(uploaded_file)\n",
    "                    print(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing PDF: {str(e)}\")\n",
    "    \n",
    "    def open_file_dialog(self):\n",
    "        \"\"\"Open system file dialog for PDF selection\"\"\"\n",
    "        file_path = filedialog.askopenfilename(\n",
    "            title=\"Select PDF file\",\n",
    "            filetypes=[(\"PDF files\", \"*.pdf\")]\n",
    "        )\n",
    "        \n",
    "        if file_path:\n",
    "            try:\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    content = file.read()\n",
    "                    \n",
    "                # Create a mock uploaded file structure similar to widget upload\n",
    "                uploaded_file = {\n",
    "                    'content': content,\n",
    "                    'metadata': {'name': os.path.basename(file_path)}\n",
    "                }\n",
    "                \n",
    "                result = self.process_uploaded_pdf(uploaded_file)\n",
    "                print(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing PDF: {str(e)}\")\n",
    "    \n",
    "    def process_uploaded_pdf(self, uploaded_file) -> str:\n",
    "        \"\"\"Process uploaded PDF file\"\"\"\n",
    "        try:\n",
    "            # Create a BytesIO object from uploaded content\n",
    "            pdf_stream = io.BytesIO(uploaded_file['content'])\n",
    "            \n",
    "            pdf_text = []\n",
    "            pdf_reader = PyPDF2.PdfReader(pdf_stream)\n",
    "            for page in pdf_reader.pages:\n",
    "                text = page.extract_text()\n",
    "                # Clean the text\n",
    "                text = re.sub(r'\\s+', ' ', text)\n",
    "                pdf_text.append(text)\n",
    "            \n",
    "            self.pdf_content = \" \".join(pdf_text)\n",
    "            \n",
    "            # Create system prompt with PDF content\n",
    "            self.system_prompt = {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"You have been provided with the following document content. \n",
    "                When answering questions, use this content as context and provide accurate responses based on it.\n",
    "                If the answer cannot be found in the document, clearly state that.\n",
    "                \n",
    "                Document content:\n",
    "                {self.pdf_content[:2000]}  # First 2000 chars for initial context\n",
    "                \n",
    "                Additional context will be provided in the conversation history.\"\"\"\n",
    "            }\n",
    "            \n",
    "            # Add chunked PDF content to conversation history\n",
    "            chunk_size = 2000\n",
    "            for i in range(0, len(self.pdf_content), chunk_size):\n",
    "                chunk = self.pdf_content[i:i + chunk_size]\n",
    "                self.conversations.append({\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"Document content continued: {chunk}\"\n",
    "                })\n",
    "            \n",
    "            return f\"PDF '{uploaded_file['metadata']['name']}' loaded successfully. Total length: {len(self.pdf_content)} characters\"\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error processing PDF: {str(e)}\")\n",
    "    \n",
    "    def add_message(self, message: str, role: str = 'user'):\n",
    "        \"\"\"Add a message to conversation history\"\"\"\n",
    "        self.conversations.append({'role': role, 'content': message})\n",
    "    \n",
    "    def generate_response(self, message: str, temperature: float = 0.7, \n",
    "                         context_window: int = 10) -> str:\n",
    "        \"\"\"Generate response using simple context window\"\"\"\n",
    "        self.add_message(message, 'user')\n",
    "        \n",
    "        # Get context messages\n",
    "        messages = []\n",
    "        if self.system_prompt:\n",
    "            messages.append(self.system_prompt)\n",
    "        \n",
    "        # Add recent conversation history\n",
    "        messages.extend(self.conversations[-context_window:])\n",
    "        \n",
    "        options = Options(\n",
    "            temperature=temperature,\n",
    "            num_ctx=8192,\n",
    "            num_predict=100,\n",
    "        )\n",
    "        \n",
    "        response = ollama.chat(\n",
    "            model=self.ollama_model,\n",
    "            messages=messages,\n",
    "            options=options\n",
    "        )\n",
    "        \n",
    "        new_message_content = response['message']['content']\n",
    "        self.add_message(new_message_content, 'assistant')\n",
    "        return new_message_content\n",
    "    \n",
    "    def view_conversation_history(self):\n",
    "        \"\"\"Display the conversation history\"\"\"\n",
    "        for i, msg in enumerate(self.conversations):\n",
    "            if msg['role'] != 'system':  # Skip system messages for cleaner output\n",
    "                print(f\"{i+1}. {msg['role'].title()}: {msg['content']}\\n\")\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear the conversation history and PDF content\"\"\"\n",
    "        self.conversations = []\n",
    "        self.pdf_content = None\n",
    "        self.system_prompt = None\n",
    "        # Clear upload widget\n",
    "        self.upload_widget.value.clear()\n",
    "        with self.status_output:\n",
    "            clear_output()\n",
    "            print(\"History and PDF content cleared!\")\n",
    "    \n",
    "    def save_conversations(self, filename: str):\n",
    "        \"\"\"Save conversations to file\"\"\"\n",
    "        import json\n",
    "        save_data = {\n",
    "            'conversations': self.conversations,\n",
    "            'pdf_content': self.pdf_content\n",
    "        }\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(save_data, f)\n",
    "    \n",
    "    def load_conversations(self, filename: str):\n",
    "        \"\"\"Load conversations from file\"\"\"\n",
    "        import json\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            self.conversations = data['conversations']\n",
    "            self.pdf_content = data['pdf_content']\n",
    "            if self.pdf_content:\n",
    "                # Recreate system prompt\n",
    "                self.system_prompt = {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"\"\"You have been provided with the following document content. \n",
    "                    When answering questions, use this content as context and provide accurate responses based on it.\n",
    "                    If the answer cannot be found in the document, clearly state that.\n",
    "                    \n",
    "                    Document content:\n",
    "                    {self.pdf_content[:2000]}\"\"\"\n",
    "                }\n",
    "\n",
    "def interactive_chat():\n",
    "    \"\"\"Initialize and run the interactive chatbot\"\"\"\n",
    "    # Initialize chatbot\n",
    "    chatbot = InteractivePDFChatbot(model_name='llama3.2:3b')\n",
    "    \n",
    "    # Display upload widget and status\n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML(\"<h3>PDF-Enhanced Chatbot</h3>\"),\n",
    "        chatbot.upload_widget,\n",
    "        chatbot.status_output\n",
    "    ]))\n",
    "    \n",
    "    print(\"Interactive Chatbot initialized! Commands:\")\n",
    "    print(\"- 'pdf': Open file dialog to select PDF\")\n",
    "    print(\"- 'quit': Exit chat\")\n",
    "    print(\"- 'history': View conversation history\")\n",
    "    print(\"- 'clear': Clear history\")\n",
    "    print(\"- 'save': Save conversation to file\")\n",
    "    print(\"- 'load': Load conversation from file\")\n",
    "    print(\"\\nUse the upload widget above or type 'pdf' to load a PDF file.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'pdf':\n",
    "            chatbot.open_file_dialog()\n",
    "            continue\n",
    "        elif user_input.lower() == 'quit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        elif user_input.lower() == 'history':\n",
    "            chatbot.view_conversation_history()\n",
    "            continue\n",
    "        elif user_input.lower() == 'clear':\n",
    "            chatbot.clear_history()\n",
    "            continue\n",
    "        elif user_input.lower() == 'save':\n",
    "            filename = input(\"Enter filename to save: \")\n",
    "            chatbot.save_conversations(filename)\n",
    "            print(f\"Conversations saved to {filename}\")\n",
    "            continue\n",
    "        elif user_input.lower() == 'load':\n",
    "            filename = input(\"Enter filename to load: \")\n",
    "            chatbot.load_conversations(filename)\n",
    "            print(f\"Conversations loaded from {filename}\")\n",
    "            continue\n",
    "        elif user_input == \"\":\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            response = chatbot.generate_response(user_input)\n",
    "            print(f\"\\nAssistant: {response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")\n",
    "\n",
    "# Run this cell to start the interactive chatbot\n",
    "interactive_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SIMULATION",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a1521ba40e7d6a6be14666f07e16a8573f7f7e33028f6e5d0c43ea71d0b845d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
